{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = list\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "assert shape([1, 2, 3]) == [3]\n",
    "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n",
    "\n",
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor.\n",
    "    Otherwise, tensor is 1-dimensonal (that is, a vector).\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "assert is_1d([1, 2, 3])\n",
    "assert not is_1d([[1, 2], [3, 4]])\n",
    "\n",
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)  # just a list of floats, use Python sum\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "\n",
    "assert tensor_sum([1, 2, 3]) == 6\n",
    "assert tensor_sum([[1, 2], [3, 4]]) == 10\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f elementwise\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
    "\n",
    "assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\n",
    "assert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]\n",
    "\n",
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "assert zeros_like([1, 2, 3]) == [0, 0, 0]\n",
    "assert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]\n",
    "\n",
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                   t1: Tensor,\n",
    "                   t2: Tensor) -> Tensor:\n",
    "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "\n",
    "import operator\n",
    "assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
    "assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Our neural networks will be composed of Layers, each of which\n",
    "    knows how to do some computation on its inputs in the \"forward\"\n",
    "    direction and propagate gradients in the \"backward\" direction.\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Note the lack of types. We're not going to be prescriptive\n",
    "        about what kinds of inputs layers can take and what kinds\n",
    "        of outputs they can return.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Similarly, we're not going to be prescriptive about what the\n",
    "        gradient looks like. It's up to you the user to make sure\n",
    "        that you're doing things sensibly.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the parameters of this layer. The default implementation\n",
    "        returns nothing, so that if you have a layer with no parameters\n",
    "        you don't have to implement this.\n",
    "        \"\"\"\n",
    "        return ()\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the gradients, in the same order as params()\n",
    "        \"\"\"\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.neural_networks import sigmoid\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply sigmoid to each element of the input tensor,\n",
    "        and save the results to use in backpropagation.\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                              self.sigmoids,\n",
    "                              gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from scratch.probability import inverse_normal_cdf\n",
    "\n",
    "def random_uniform(*dims: int) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
    "\n",
    "def random_normal(*dims: int,\n",
    "                  mean: float = 0.0,\n",
    "                  variance: float = 1.0) -> Tensor:\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * inverse_normal_cdf(random.random())\n",
    "                for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
    "                for _ in range(dims[0])]\n",
    "\n",
    "assert shape(random_uniform(2, 3, 4)) == [2, 3, 4]\n",
    "assert shape(random_normal(5, 6, mean=10)) == [5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
    "    if init == 'normal':\n",
    "        return random_normal(*dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(*dims)\n",
    "    elif init == 'xavier':\n",
    "        variance = len(dims) / sum(dims)\n",
    "        return random_normal(*dims, variance=variance)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown init: {init}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import dot\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n",
    "        \"\"\"\n",
    "        A layer of output_dim neurons, each with input_dim weights\n",
    "        (and a bias).\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # self.w[o] is the weights for the o-th neuron\n",
    "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
    "\n",
    "        # self.b[o] is the bias term for the o-th neuron\n",
    "        self.b = random_tensor(output_dim, init=init)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save the input to use in the backward pass.\n",
    "        self.input = input\n",
    "\n",
    "        # Return the vector of neuron outputs.\n",
    "        return [dot(input, self.w[o]) + self.b[o]\n",
    "                for o in range(self.output_dim)]\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        # Each b[o] gets added to output[o], which means\n",
    "        # the gradient of b is the same as the output gradient.\n",
    "        self.b_grad = gradient\n",
    "\n",
    "        # Each w[o][i] multiplies input[i] and gets added to output[o].\n",
    "        # So its gradient is input[i] * gradient[o].\n",
    "        self.w_grad = [[self.input[i] * gradient[o]\n",
    "                        for i in range(self.input_dim)]\n",
    "                       for o in range(self.output_dim)]\n",
    "\n",
    "        # Each input[i] multiplies every w[o][i] and gets added to every\n",
    "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n",
    "        # across all the outputs.\n",
    "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
    "                for i in range(self.input_dim)]\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        return [self.w, self.b]\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        return [self.w_grad, self.b_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \"\"\"\n",
    "    A layer consisting of a sequence of other layers.\n",
    "    It's up to you to make sure that the output of each layer\n",
    "    makes sense as the input to the next layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: List[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Just forward the input through the layers in order.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "        return gradient\n",
    "\n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the params from each layer.\"\"\"\n",
    "        return (param for layer in self.layers for param in layer.params())\n",
    "\n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"Just return the grads from each layer.\"\"\"\n",
    "        return (grad for layer in self.layers for grad in layer.grads())\n",
    "\n",
    "class Loss:\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        \"\"\"How does the loss change as the predictions change?\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SSE(Loss):\n",
    "    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        # Compute the tensor of squared differences\n",
    "        squared_errors = tensor_combine(\n",
    "            lambda predicted, actual: (predicted - actual) ** 2,\n",
    "            predicted,\n",
    "            actual)\n",
    "\n",
    "        # And just add them up\n",
    "        return tensor_sum(squared_errors)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda predicted, actual: 2 * (predicted - actual),\n",
    "            predicted,\n",
    "            actual)\n",
    "    \n",
    "sse_loss = SSE()\n",
    "assert sse_loss.loss([1, 2, 3], [10, 20, 30]) == 9 ** 2 + 18 ** 2 + 27 ** 2\n",
    "assert sse_loss.gradient([1, 2, 3], [10, 20, 30]) == [-18, -36, -54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    An optimizer updates the weights of a layer (in place) using information\n",
    "    known by either the layer or the optimizer (or by both).\n",
    "    \"\"\"\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescent(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.1) -> None:\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        for param, grad in zip(layer.params(), layer.grads()):\n",
    "            # Update param using a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda param, grad: param - grad * self.lr,\n",
    "                param,\n",
    "                grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = [[1, 2], [3, 4]]\n",
    "\n",
    "for row in tensor:\n",
    "    row = [0, 0]\n",
    "assert tensor == [[1, 2], [3, 4]], \"assignment doesn't update a list\"\n",
    "\n",
    "for row in tensor:\n",
    "    row[:] = [0, 0]\n",
    "assert tensor == [[0, 0], [0, 0]], \"but slice assignment does\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    def __init__(self,\n",
    "                 learning_rate: float,\n",
    "                 momentum: float = 0.9) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.mo = momentum\n",
    "        self.updates: List[Tensor] = []  # running average\n",
    "\n",
    "    def step(self, layer: Layer) -> None:\n",
    "        # If we have no previous updates, start with all zeros.\n",
    "        if not self.updates:\n",
    "            self.updates = [zeros_like(grad) for grad in layer.grads()]\n",
    "\n",
    "        for update, param, grad in zip(self.updates,\n",
    "                                       layer.params(),\n",
    "                                       layer.grads()):\n",
    "            # Apply momentum\n",
    "            update[:] = tensor_combine(\n",
    "                lambda u, g: self.mo * u + (1 - self.mo) * g,\n",
    "                update,\n",
    "                grad)\n",
    "\n",
    "            # Then take a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda p, u: p - self.lr * u,\n",
    "                param,\n",
    "                update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tanh(x: float) -> float:\n",
    "    # If x is very large or very small, tanh is (essentially) 1 or -1.\n",
    "    # We check for this because e.g. math.exp(1000) raises an error.\n",
    "    if x < -100:  return -1\n",
    "    elif x > 100: return 1\n",
    "\n",
    "    em2x = math.exp(-2 * x)\n",
    "    return (1 - em2x) / (1 + em2x)\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Save tanh output to use in backward pass.\n",
    "        self.tanh = tensor_apply(tanh, input)\n",
    "        return self.tanh\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(\n",
    "            lambda tanh, grad: (1 - tanh ** 2) * grad,\n",
    "            self.tanh,\n",
    "            gradient)\n",
    "\n",
    "class Relu(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        self.input = input\n",
    "        return tensor_apply(lambda x: max(x, 0), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n",
    "                              self.input,\n",
    "                              gradient)\n",
    "\n",
    "def softmax(tensor: Tensor) -> Tensor:\n",
    "    \"\"\"Softmax along the last dimension\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        # Subtract largest value for numerical stabilitity.\n",
    "        largest = max(tensor)\n",
    "        exps = [math.exp(x - largest) for x in tensor]\n",
    "\n",
    "        sum_of_exps = sum(exps)                 # This is the total \"weight\".\n",
    "        return [exp_i / sum_of_exps             # Probability is the fraction\n",
    "                for exp_i in exps]              # of the total weight.\n",
    "    else:\n",
    "        return [softmax(tensor_i) for tensor_i in tensor]\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    \"\"\"\n",
    "    This is the negative-log-likelihood of the observed values, given the\n",
    "    neural net model. So if we choose weights to minimize it, our model will\n",
    "    be maximizing the likelihood of the observed data.\n",
    "    \"\"\"\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        # This will be log p_i for the actual class i and 0 for the other\n",
    "        # classes. We add a tiny amount to p to avoid taking log(0).\n",
    "        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n",
    "                                     probabilities,\n",
    "                                     actual)\n",
    "\n",
    "        # And then we just sum up the negatives.\n",
    "        return -tensor_sum(likelihoods)\n",
    "\n",
    "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        # Isn't this a pleasant equation?\n",
    "        return tensor_combine(lambda p, actual: p - actual,\n",
    "                              probabilities,\n",
    "                              actual)\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p: float) -> None:\n",
    "        self.p = p\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Create a mask of 0s and 1s shaped like the input\n",
    "            # using the specified probability.\n",
    "            self.mask = tensor_apply(\n",
    "                lambda _: 0 if random.random() < self.p else 1,\n",
    "                input)\n",
    "            # Multiply by the mask to dropout inputs.\n",
    "            return tensor_combine(operator.mul, input, self.mask)\n",
    "        else:\n",
    "            # During evaluation just scale down the outputs uniformly.\n",
    "            return tensor_apply(lambda x: x * (1 - self.p), input)\n",
    "\n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        if self.train:\n",
    "            # Only propagate the gradients where mask == 1\n",
    "            return tensor_combine(operator.mul, gradient, self.mask)\n",
    "        else:\n",
    "            raise RuntimeError(\"don't call backward when not in train mode\")\n",
    "\n",
    "\n",
    "#plt.savefig('im/mnist.png')\n",
    "#plt.gca().clear()\n",
    "\n",
    "def one_hot_encode(i: int, num_labels: int = 10) -> List[float]:\n",
    "    return [1.0 if j == i else 0.0 for j in range(num_labels)]\n",
    "\n",
    "assert one_hot_encode(3) == [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "assert one_hot_encode(2, num_labels=5) == [0, 0, 1, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import squared_distance\n",
    "\n",
    "import json\n",
    "\n",
    "def save_weights(model: Layer, filename: str) -> None:\n",
    "    weights = list(model.params())\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(weights, f)\n",
    "\n",
    "def load_weights(model: Layer, filename: str) -> None:\n",
    "    with open(filename) as f:\n",
    "        weights = json.load(f)\n",
    "\n",
    "    # Check for consistency\n",
    "    assert all(shape(param) == shape(weight)\n",
    "               for param, weight in zip(model.params(), weights))\n",
    "\n",
    "    # Then load using slice assignment:\n",
    "    for param, weight in zip(model.params(), weights):\n",
    "        param[:] = weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xor loss 0.000: 100%|██████████| 3000/3000 [00:02<00:00, 1033.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.6425160695224108, -1.4948117798303164], [-4.56764657202966, -3.364917635073195]]\n",
      "[1.7673716823255199, 0.38727014379472763]\n",
      "[[3.1986204791704025, -3.5018030621426197]]\n",
      "[-0.6462765963362209]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    # XOR revisited\n",
    "    \n",
    "    # training data\n",
    "    xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "    ys = [[0.], [1.], [1.], [0.]]\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    net = Sequential([\n",
    "        Linear(input_dim=2, output_dim=2),\n",
    "        Sigmoid(),\n",
    "        Linear(input_dim=2, output_dim=1)\n",
    "    ])\n",
    "    \n",
    "    import tqdm\n",
    "    \n",
    "    optimizer = GradientDescent(learning_rate=0.1)\n",
    "    loss = SSE()\n",
    "    \n",
    "    with tqdm.trange(3000) as t:\n",
    "        for epoch in t:\n",
    "            epoch_loss = 0.0\n",
    "    \n",
    "            for x, y in zip(xs, ys):\n",
    "                predicted = net.forward(x)\n",
    "                epoch_loss += loss.loss(predicted, y)\n",
    "                gradient = loss.gradient(predicted, y)\n",
    "                net.backward(gradient)\n",
    "    \n",
    "                optimizer.step(net)\n",
    "    \n",
    "            t.set_description(f\"xor loss {epoch_loss:.3f}\")\n",
    "    \n",
    "    for param in net.params():\n",
    "        print(param)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fb loss: 64.55 acc: 0.95: 100%|██████████| 1000/1000 [06:57<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "    # FizzBuzz Revisited\n",
    "    \n",
    "    from scratch.neural_networks import binary_encode, fizz_buzz_encode, argmax\n",
    "    \n",
    "    xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "    ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
    "    \n",
    "    NUM_HIDDEN = 25\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    net = Sequential([\n",
    "        Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
    "        Tanh(),\n",
    "        Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform'),\n",
    "        Sigmoid()\n",
    "    ])\n",
    "    \n",
    "    def fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float:\n",
    "        num_correct = 0\n",
    "        for n in range(low, hi):\n",
    "            x = binary_encode(n)\n",
    "            predicted = argmax(net.forward(x))\n",
    "            actual = argmax(fizz_buzz_encode(n))\n",
    "            if predicted == actual:\n",
    "                num_correct += 1\n",
    "    \n",
    "        return num_correct / (hi - low)\n",
    "    \n",
    "    optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
    "    loss = SSE()\n",
    "    \n",
    "    with tqdm.trange(1000) as t:\n",
    "        for epoch in t:\n",
    "            epoch_loss = 0.0\n",
    "    \n",
    "            for x, y in zip(xs, ys):\n",
    "                predicted = net.forward(x)\n",
    "                epoch_loss += loss.loss(predicted, y)\n",
    "                gradient = loss.gradient(predicted, y)\n",
    "                net.backward(gradient)\n",
    "    \n",
    "                optimizer.step(net)\n",
    "    \n",
    "            accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
    "            t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fb loss: 5.288 acc: 1.00: 100%|██████████| 100/100 [00:41<00:00,  2.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    # Now check results on the test set\n",
    "    print(\"test results\", fizzbuzz_accuracy(1, 101, net))\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    net = Sequential([\n",
    "        Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
    "        Tanh(),\n",
    "        Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform')\n",
    "        # No final sigmoid layer now\n",
    "    ])\n",
    "    \n",
    "    optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
    "    loss = SoftmaxCrossEntropy()\n",
    "    \n",
    "    with tqdm.trange(100) as t:\n",
    "        for epoch in t:\n",
    "            epoch_loss = 0.0\n",
    "    \n",
    "            for x, y in zip(xs, ys):\n",
    "                predicted = net.forward(x)\n",
    "                epoch_loss += loss.loss(predicted, y)\n",
    "                gradient = loss.gradient(predicted, y)\n",
    "                net.backward(gradient)\n",
    "    \n",
    "                optimizer.step(net)\n",
    "    \n",
    "            accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
    "            t.set_description(f\"fb loss: {epoch_loss:.3f} acc: {accuracy:.2f}\")\n",
    "    \n",
    "    # Again check results on the test set\n",
    "    print(\"test results\", fizzbuzz_accuracy(1, 101, net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kaoru/Documents/Python_ゼロからはじめるデータサイエンス/data-science-from-scratch'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "# データ取得\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.tolist()\n",
    "train_labels = train_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    #import mnist\n",
    "    \n",
    "    # This will download the data, change this to where you want it.\n",
    "    # (Yes, it's a 0-argument function, that's what the library expects.)\n",
    "    # (Yes, I'm assigning a lambda to a variable, like I said never to do.)\n",
    "    #mnist.temporary_dir = lambda: '/tmp'\n",
    "    \n",
    "    # Each of these functions first downloads the data and returns a numpy array.\n",
    "    # We call .tolist() because our \"tensors\" are just lists.\n",
    "    #train_images = mnist.train_images().tolist()\n",
    "    #train_labels = mnist.train_labels().tolist()\n",
    "    \n",
    "    assert shape(train_images) == [60000, 28, 28]\n",
    "    assert shape(train_labels) == [60000]\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, ax = plt.subplots(10, 10)\n",
    "    \n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            # Plot each image in black and white and hide the axes.\n",
    "            ax[i][j].imshow(train_images[10 * i + j], cmap='Greys')\n",
    "            ax[i][j].xaxis.set_visible(False)\n",
    "            ax[i][j].yaxis.set_visible(False)\n",
    "    \n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "    # Load the MNIST test data\n",
    "    \n",
    "    test_images = test_images.tolist()\n",
    "    test_labels = test_labels.tolist()\n",
    "    \n",
    "    assert shape(test_images) == [10000, 28, 28]\n",
    "    assert shape(test_labels) == [10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Recenter the images\n",
    "    \n",
    "    # Compute the average pixel value\n",
    "    avg = tensor_sum(train_images) / 60000 / 28 / 28\n",
    "    \n",
    "    # Recenter, rescale, and flatten\n",
    "    train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "                    for image in train_images]\n",
    "    test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
    "                   for image in test_images]\n",
    "    \n",
    "    assert shape(train_images) == [60000, 784], \"images should be flattened\"\n",
    "    assert shape(test_images) == [10000, 784], \"images should be flattened\"\n",
    "    \n",
    "    # After centering, average pixel should be very close to 0\n",
    "    assert -0.0001 < tensor_sum(train_images) < 0.0001\n",
    "    \n",
    "    \n",
    "    # One-hot encode the test data\n",
    "    \n",
    "    train_labels = [one_hot_encode(label) for label in train_labels]\n",
    "    test_labels = [one_hot_encode(label) for label in test_labels]\n",
    "    \n",
    "    assert shape(train_labels) == [60000, 10]\n",
    "    assert shape(test_labels) == [10000, 10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Training loop\n",
    "    \n",
    "    import tqdm\n",
    "    \n",
    "    def loop(model: Layer,\n",
    "             images: List[Tensor],\n",
    "             labels: List[Tensor],\n",
    "             loss: Loss,\n",
    "             optimizer: Optimizer = None) -> None:\n",
    "        correct = 0         # Track number of correct predictions.\n",
    "        total_loss = 0.0    # Track total loss.\n",
    "    \n",
    "        with tqdm.trange(len(images)) as t:\n",
    "            for i in t:\n",
    "                predicted = model.forward(images[i])             # Predict.\n",
    "                if argmax(predicted) == argmax(labels[i]):       # Check for\n",
    "                    correct += 1                                 # correctness.\n",
    "                total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n",
    "    \n",
    "                # If we're training, backpropagate gradient and update weights.\n",
    "                if optimizer is not None:\n",
    "                    gradient = loss.gradient(predicted, labels[i])\n",
    "                    model.backward(gradient)\n",
    "                    optimizer.step(model)\n",
    "    \n",
    "                # And update our metrics in the progress bar.\n",
    "                avg_loss = total_loss / (i + 1)\n",
    "                acc = correct / (i + 1)\n",
    "                t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mnist loss: 0.357 acc: 0.898: 100%|██████████| 60000/60000 [08:02<00:00, 124.27it/s]\n",
      "mnist loss: 0.361 acc: 0.891: 100%|██████████| 10000/10000 [00:17<00:00, 586.55it/s]\n"
     ]
    }
   ],
   "source": [
    "    # The logistic regression model for MNIST\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    # Logistic regression is just a linear layer followed by softmax\n",
    "    model = Linear(784, 10)\n",
    "    loss = SoftmaxCrossEntropy()\n",
    "    \n",
    "    # This optimizer seems to work\n",
    "    optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "    \n",
    "    # Train on the training data\n",
    "    loop(model, train_images, train_labels, loss, optimizer)\n",
    "    \n",
    "    # Test on the test data (no optimizer means just evaluate)\n",
    "    loop(model, test_images, test_labels, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # A deep neural network for MNIST\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    # Name them so we can turn train on and off\n",
    "    dropout1 = Dropout(0.1)\n",
    "    dropout2 = Dropout(0.1)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Linear(784, 30),  # Hidden layer 1: size 30\n",
    "        dropout1,\n",
    "        Tanh(),\n",
    "        Linear(30, 10),   # Hidden layer 2: size 10\n",
    "        dropout2,\n",
    "        Tanh(),\n",
    "        Linear(10, 10)    # Output layer: size 10\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mnist loss: 0.486 acc: 0.857: 100%|██████████| 60000/60000 [21:53<00:00, 45.69it/s]\n",
      "mnist loss: 0.313 acc: 0.908: 100%|██████████| 10000/10000 [00:30<00:00, 331.04it/s]\n"
     ]
    }
   ],
   "source": [
    "    # Training the deep model for MNIST\n",
    "    \n",
    "    optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
    "    loss = SoftmaxCrossEntropy()\n",
    "    \n",
    "    # Enable dropout and train (takes > 20 minutes on my laptop!)\n",
    "    dropout1.train = dropout2.train = True\n",
    "    loop(model, train_images, train_labels, loss, optimizer)\n",
    "    \n",
    "    # Disable dropout and evaluate\n",
    "    dropout1.train = dropout2.train = False\n",
    "    loop(model, test_images, test_labels, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
